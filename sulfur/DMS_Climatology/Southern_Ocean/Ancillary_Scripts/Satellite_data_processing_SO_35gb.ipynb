{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Satellite_data_processing_SO_35gb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bjmcnabb/DMS_Climatology/blob/main/Southern%20Ocean/Satellite_data_processing_SO_35gb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tAb77yZ9fzMG",
        "outputId": "41fcc402-dad7-4902-f828-033553ac84e4"
      },
      "source": [
        "\"\"\"\n",
        "Created on Tue Oct 27 20:19:13 2020\n",
        "\n",
        "@author: Brandon McNabb (bmcnabb@eoas.ubc.ca)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nCreated on Tue Oct 27 20:19:13 2020\\n\\n@author: Brandon McNabb (bmcnabb@eoas.ubc.ca)\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pafL7Li0jyXW",
        "outputId": "d0b3b32b-581b-4906-e84d-ebc095a38b2f"
      },
      "source": [
        "! pip install dask[dataframe]\n",
        "! pip install obspy\n",
        "! pip install pyhdf\n",
        "! pip install numpy --upgrade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dask[dataframe] in /usr/local/lib/python3.7/dist-packages (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]) (1.1.5)\n",
            "Collecting fsspec>=0.6.0\n",
            "  Downloading fsspec-2021.11.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 12.4 MB/s \n",
            "\u001b[?25hCollecting partd>=0.3.10\n",
            "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: toolz>=0.7.3 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]) (0.11.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->dask[dataframe]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->dask[dataframe]) (2018.9)\n",
            "Collecting locket\n",
            "  Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.0->dask[dataframe]) (1.15.0)\n",
            "Installing collected packages: locket, partd, fsspec\n",
            "Successfully installed fsspec-2021.11.0 locket-0.2.1 partd-1.2.0\n",
            "Collecting obspy\n",
            "  Downloading obspy-1.2.2.zip (24.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.7 MB 1.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future>=0.12.4 in /usr/local/lib/python3.7/dist-packages (from obspy) (0.16.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from obspy) (4.2.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from obspy) (57.4.0)\n",
            "Requirement already satisfied: matplotlib>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from obspy) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from obspy) (1.19.5)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from obspy) (1.4.26)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from obspy) (4.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from obspy) (2.23.0)\n",
            "Requirement already satisfied: scipy>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from obspy) (1.4.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.1.0->obspy) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.1.0->obspy) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.1.0->obspy) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.1.0->obspy) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=1.1.0->obspy) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->obspy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->obspy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->obspy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->obspy) (2.10)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->obspy) (4.8.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->obspy) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy->obspy) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy->obspy) (3.6.0)\n",
            "Building wheels for collected packages: obspy\n",
            "  Building wheel for obspy (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for obspy: filename=obspy-1.2.2-cp37-cp37m-linux_x86_64.whl size=21668551 sha256=06510e31a26667b00f5a3b916b9df9ded698be34fb53593f65678db93a7d0387\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/7e/ea/0a37d5f5001d096cf97d6527b60300badd2d0074449e89c736\n",
            "Successfully built obspy\n",
            "Installing collected packages: obspy\n",
            "Successfully installed obspy-1.2.2\n",
            "Collecting pyhdf\n",
            "  Downloading pyhdf-0.10.3-cp37-cp37m-manylinux2014_x86_64.whl (757 kB)\n",
            "\u001b[K     |████████████████████████████████| 757 kB 12.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyhdf) (1.19.5)\n",
            "Installing collected packages: pyhdf\n",
            "Successfully installed pyhdf-0.10.3\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Collecting numpy\n",
            "  Using cached numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.21.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JapQp4HxQgM1",
        "outputId": "a2ae732e-ded9-46c3-d681-0653c57d1617"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# !unzip drive/MyDrive/SO_DMS_predictors.zip\n",
        "!unzip drive/MyDrive/CCMP_0.25_wind_speeds.zip\n",
        "# !unzip drive/MyDrive/WOA18.zip\n",
        "!unzip drive/MyDrive/CERSAT-GLO-REP_WIND_L4-OBS_FULL_TIME_SERIE_1607246388558.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Archive:  drive/MyDrive/CCMP_0.25_wind_speeds.zip\n",
            "   creating: CCMP_0.25_wind_speeds/\n",
            " extracting: CCMP_0.25_wind_speeds/month_19971201_v11l35flk.nc.gz  \n",
            " extracting: CCMP_0.25_wind_speeds/month_20011201_v11l35flk.nc.gz  \n",
            " extracting: CCMP_0.25_wind_speeds/month_20020101_v11l35flk.nc.gz  \n",
            " extracting: CCMP_0.25_wind_speeds/month_20020201_v11l35flk.nc.gz  \n",
            " extracting: CCMP_0.25_wind_speeds/month_20030101_v11l35flk.nc.gz  \n",
            " extracting: CCMP_0.25_wind_speeds/month_20030201_v11l35flk.nc.gz  \n",
            "  inflating: CCMP_0.25_wind_speeds/month_20030301_v11l35flk.nc  \n",
            "  inflating: CCMP_0.25_wind_speeds/month_20031001_v11l35flk.nc  \n",
            " extracting: CCMP_0.25_wind_speeds/month_20031101_v11l35flk.nc.gz  \n",
            " extracting: CCMP_0.25_wind_speeds/month_20040101_v11l35flk.nc.gz  \n",
            " extracting: CCMP_0.25_wind_speeds/month_20040201_v11l35flk.nc.gz  \n",
            " extracting: CCMP_0.25_wind_speeds/month_20041201_v11l35flk.nc.gz  \n",
            " extracting: CCMP_0.25_wind_speeds/month_20050101_v11l35flk.nc.gz  \n",
            " extracting: CCMP_0.25_wind_speeds/month_20050201_v11l35flk.nc.gz  \n",
            "  inflating: CCMP_0.25_wind_speeds/month_20051001_v11l35flk.nc  \n",
            " extracting: CCMP_0.25_wind_speeds/month_20051101_v11l35flk.nc.gz  \n",
            " extracting: CCMP_0.25_wind_speeds/month_20051201_v11l35flk.nc.gz  \n",
            " extracting: CCMP_0.25_wind_speeds/month_20060101_v11l35flk.nc.gz  \n",
            " extracting: CCMP_0.25_wind_speeds/month_20060201_v11l35flk.nc.gz  \n",
            " extracting: CCMP_0.25_wind_speeds/month_20061101_v11l35flk.nc.gz  \n",
            " extracting: CCMP_0.25_wind_speeds/month_20061201_v11l35flk.nc.gz  \n",
            "  inflating: CCMP_0.25_wind_speeds/month_20080301_v11l35flk.nc  \n",
            "  inflating: CCMP_0.25_wind_speeds/month_20080401_v11l35flk.nc  \n",
            "Archive:  drive/MyDrive/CERSAT-GLO-REP_WIND_L4-OBS_FULL_TIME_SERIE_1607246388558.zip\n",
            "  inflating: CERSAT-GLO-REP_WIND_L4-OBS_FULL_TIME_SERIE_1607246388558.nc  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS5mRibHGj_J"
      },
      "source": [
        "# Process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yXE4ieH21aa"
      },
      "source": [
        "import timeit\n",
        "runtime_start = timeit.default_timer() # start the clock"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmjTi7Zn26Za"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import scipy.interpolate\n",
        "import os\n",
        "from dateutil import parser\n",
        "import xarray as xr\n",
        "import dask.array as da\n",
        "import dask.dataframe as dd\n",
        "from pyhdf.SD import SD, SDC\n",
        "import datetime\n",
        "from obspy.geodetics import kilometers2degrees"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gfkM42829bV"
      },
      "source": [
        "#~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
        "# *** Make sure lat/lons bounds are the same as analysis script!! ***\n",
        "#~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
        "\n",
        "#### Spatial grid resolution (degrees):\n",
        "grid = kilometers2degrees(20) # convert km to deg\n",
        "\n",
        "#### Define lat/lon constraints\n",
        "\n",
        "# set spatial bounding limits\n",
        "min_lon, max_lon, min_lat, max_lat = -180, 180, -90, -40\n",
        "\n",
        "#### Define bins\n",
        "latbins = np.arange(min_lat,max_lat+grid,grid)\n",
        "lonbins = np.arange(min_lon,max_lon+grid,grid)\n",
        "\n",
        "#### Set a call for which dataset is being processed\n",
        "which_dataset = 'DMS'\n",
        "# which_dataset = 'Fe'\n",
        "if which_dataset == 'DMS':\n",
        "  write_to = 'SO_DMS_'\n",
        "else:\n",
        "  write_to = 'SO_Fe_'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTfw6EVr2-fO",
        "outputId": "fe7ed261-89d0-452f-ed0d-d958b55421ae"
      },
      "source": [
        "start = timeit.default_timer() # start the clock\n",
        "#-----------------------------------------------------------------------------\n",
        "# Set path:\n",
        "# file directory shape:\n",
        "# -/MODIS\n",
        "#       |-/Data\n",
        "#           |-/1_Chl\n",
        "#               |-/*.nc\n",
        "#           |-/2_PIC\n",
        "#               |-/*.nc\n",
        "#           |-/3_SST\n",
        "#               |-/*.nc\n",
        "#           |-/4_PAR\n",
        "#               |-/*.nc\n",
        "#           |-/5_Kd\n",
        "#               |-/*.nc\n",
        "#           |-/6_POC\n",
        "#               |-/*.nc\n",
        "#           |-/7_FLH\n",
        "#               |-/*.nc\n",
        "#           |-/8_SSHA\n",
        "#               |-/*.nc\n",
        "#           |-/9_NPP\n",
        "#               |-/*.hdf\n",
        "#           |-/10_MIMOC_MLD\n",
        "#               |-/*.hdf\n",
        "#           |-/11_MIMOC_TS\n",
        "#               |-/*.hdf\n",
        "\n",
        "# Set directory to write data files to:\n",
        "write_dir = 'C:/Users/bcamc/OneDrive/Desktop/Python/Projects/Sulfur/Southern Ocean/SO_DMS_data_longer_ts'\n",
        "# Call directory with raw data files to process\n",
        "# directory = 'E:/Satellite/SO_DMS_predictors' # E: or C:, depending if using hard drive/thumb drive\n",
        "directory = 'SO_DMS_predictors/' # E: or C:, depending if using hard drive/thumb drive\n",
        "folders = sorted(os.listdir(directory)) # calls specific datasets grouped in folders\n",
        "num_folders = len(folders) # set this to the number of variables\n",
        "#-----------------------------------------------------------------------------\n",
        "\n",
        "print('finding dimensions...')\n",
        "\n",
        "# ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
        "# preallocate variable:\n",
        "dims = np.empty((num_folders,4))\n",
        "# ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
        "for count, folder in enumerate(folders):\n",
        "    path = os.path.join(directory, folder)\n",
        "    for subdir, dirs, files in os.walk(path):\n",
        "        for n, file in enumerate(files):\n",
        "            if count < 9 and n == 1:\n",
        "                if count == 0 and n == 1:\n",
        "                    # folowing pulls the number of unique months within the directory files; used to estimate total data length for binning\n",
        "                    num_months = np.size(np.unique([pd.to_datetime(datetime.datetime.strptime(FILE.split('.')[0][1:8], '%Y%j').date()).strftime('%m') for FILE in files]))\n",
        "                # find lat/lon dimensions:\n",
        "                # ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
        "                # Open up one of the datasets\n",
        "                filepath = subdir + os.sep + file\n",
        "                check = xr.open_dataset(filepath)\n",
        "                # Pull the lat/lon from it:\n",
        "                lat = check.coords['lat'].values\n",
        "                lon = check.coords['lon'].values\n",
        "                # First find the indices to index by lat/lon\n",
        "                latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "                loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "                # Convert time/lats/lons into repeating matrices to match data dimensions\n",
        "                lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
        "                lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
        "                # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
        "                # Need these as pandas dataframes to using binning scheme:\n",
        "                # NOTE: dummy data added to 'datetime' & 'data' columns to perserve shape \n",
        "                d = {'datetime':np.ravel(lat_mat),'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(lat_mat)}\n",
        "                data_long = pd.DataFrame(data=d)\n",
        "                # Bin data as averages across gridded spatial bins:\n",
        "                to_bin = lambda x: np.round(x / grid) * grid\n",
        "                data_long['latbins'] = data_long.lat.map(to_bin)\n",
        "                data_long['lonbins'] = data_long.lon.map(to_bin)\n",
        "                data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
        "                # Rename binned columns + drop mean lat/lons:\n",
        "                data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
        "                data_proc.reset_index(inplace=True) # remove index specification on columns\n",
        "                check.close()\n",
        "                del check\n",
        "            if count == 9 and n == 1: # Data configured differently for SSHA data\n",
        "                # folowing pulls the number of unique months within the directory files; used to estimate total data length for binning\n",
        "                num_months = np.size(np.unique([pd.to_datetime(datetime.datetime.strptime(FILE.split('_')[3][:8], '%Y%m%d').date()).strftime('%m') for FILE in files]))\n",
        "                # find lat/lon dimensions:\n",
        "                # ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
        "                # Open up one of the datasets\n",
        "                filepath = subdir + os.sep + file\n",
        "                check = xr.open_dataset(filepath)\n",
        "                # Pull the lat/lon from it:\n",
        "                lat = check.coords['Latitude'].values\n",
        "                lon = check.coords['Longitude'].values\n",
        "                # First find the indices to index by lat/lon\n",
        "                latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "                loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "                # Convert time/lats/lons into repeating matrices to match data dimensions\n",
        "                lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
        "                lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
        "                # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
        "                # Need these as pandas dataframes to using binning scheme:\n",
        "                # NOTE: dummy data added to 'datetime' & 'data' columns to perserve shape \n",
        "                d = {'datetime':np.ravel(lat_mat),'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(lat_mat)}\n",
        "                data_long = pd.DataFrame(data=d)\n",
        "                # Bin data as averages across gridded spatial bins:\n",
        "                to_bin = lambda x: np.round(x / grid) * grid\n",
        "                data_long['latbins'] = data_long.lat.map(to_bin)\n",
        "                data_long['lonbins'] = data_long.lon.map(to_bin)\n",
        "                data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
        "                # Rename binned columns + drop mean lat/lons:\n",
        "                data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
        "                data_proc.reset_index(inplace=True) # remove index specification on columns\n",
        "                check.close()\n",
        "                del check\n",
        "            if count == 10 and n == 1: # Data configured differently for NPP data\n",
        "                # folowing pulls the number of unique months within the directory files; used to estimate total data length for binning\n",
        "                num_months = np.size(np.unique([pd.to_datetime(datetime.datetime.strptime(FILE.split('.')[1][:7], '%Y%j').date()).strftime('%m') for FILE in files]))\n",
        "                # find lat/lon dimensions:\n",
        "                # ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
        "                # Open up one of the datasets\n",
        "                filepath_NPP = subdir + os.sep + file\n",
        "                vars_ = SD(filepath_NPP, SDC.READ)\n",
        "                # data = np.flipud(vars_.select('npp').get())\n",
        "                # lets pull the lat/lon from it:\n",
        "                n = 180/1080\n",
        "                lat = np.arange(-90,90,n)\n",
        "                lon = np.arange(-180,180,n)\n",
        "                # First find the indices to index by lat/lon\n",
        "                latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "                loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "                # Convert time/lats/lons into repeating matrices to match data dimensions\n",
        "                lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
        "                lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
        "                # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
        "                # Need these as pandas dataframes to using binning scheme:\n",
        "                # NOTE: dummy data added to 'datetime' & 'data' columns to perserve shape \n",
        "                d = {'datetime':np.ravel(lat_mat),'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(lat_mat)}\n",
        "                data_long = pd.DataFrame(data=d)\n",
        "                # Bin data as averages across gridded spatial bins:\n",
        "                to_bin = lambda x: np.round(x / grid) * grid\n",
        "                data_long['latbins'] = data_long.lat.map(to_bin)\n",
        "                data_long['lonbins'] = data_long.lon.map(to_bin)\n",
        "                data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
        "                # Rename binned columns + drop mean lat/lons:\n",
        "                data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
        "                data_proc.reset_index(inplace=True) # remove index specification on columns\n",
        "                del vars_\n",
        "            if count == 11 or count == 11 and n == 1: # Data configured differently for MLD data\n",
        "                # folowing pulls the number of unique months within the directory files; used to estimate total data length for binning\n",
        "                num_months = np.size([FILE.split(\"month\")[1].split('.nc')[0] for FILE in files])\n",
        "                # find lat/lon dimensions:\n",
        "                # ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
        "                # Open up one of the datasets\n",
        "                filepath = subdir + os.sep + file\n",
        "                check = xr.open_dataset(filepath)\n",
        "                # lets pull the lat/lon from it:\n",
        "                lat = check.LATITUDE.values\n",
        "                lon = check.LONGITUDE.values-180 # convert to W/E coords\n",
        "                # First find the indices to index by lat/lon\n",
        "                latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "                loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "                # Convert time/lats/lons into repeating matrices to match data dimensions\n",
        "                lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
        "                lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
        "                # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
        "                # Need these as pandas dataframes to using binning scheme:\n",
        "                # NOTE: dummy data added to 'datetime' & 'data' columns to perserve shape \n",
        "                d = {'datetime':np.ravel(lat_mat),'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(lat_mat)}\n",
        "                data_long = pd.DataFrame(data=d)\n",
        "                # Bin data as averages across gridded spatial bins:\n",
        "                to_bin = lambda x: np.round(x / grid) * grid\n",
        "                data_long['latbins'] = data_long.lat.map(to_bin)\n",
        "                data_long['lonbins'] = data_long.lon.map(to_bin)\n",
        "                data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
        "                # Rename binned columns + drop mean lat/lons:\n",
        "                data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
        "                data_proc.reset_index(inplace=True) # remove index specification on columns\n",
        "                check.close()\n",
        "                del check\n",
        "            else:\n",
        "                pass\n",
        "    # extract number of files (datasets) per folder + lat/lon dimensions:\n",
        "    dims[count] = np.array([int(count), int(np.size(files)), int(data_proc.shape[0]*num_months), int(data_proc.shape[1])])\n",
        "    print('\\nData length:',str(data_proc.shape[0]))\n",
        "    print('In folder ' + '\"' + folder + '\":')\n",
        "    print('Number of datasets = '+ str(np.size(files)))\n",
        "    print('Number of unique months = '+ str(num_months))\n",
        "end = timeit.default_timer() # stop the clock\n",
        "print('\\n~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~')\n",
        "print('Execution time:')\n",
        "print(str(round(end-start,5)),'secs')\n",
        "print(str(round((end-start)/60,5)),'mins')\n",
        "print(str(round((end-start)/3600,5)),'hrs')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finding dimensions...\n",
            "\n",
            "Data length: 2403600\n",
            "In folder \"1_Chl\":\n",
            "Number of datasets = 36\n",
            "Number of unique months = 7\n",
            "\n",
            "Data length: 2403600\n",
            "In folder \"2_PIC\":\n",
            "Number of datasets = 36\n",
            "Number of unique months = 7\n",
            "\n",
            "Data length: 2403600\n",
            "In folder \"3_SST\":\n",
            "Number of datasets = 36\n",
            "Number of unique months = 7\n",
            "\n",
            "Data length: 1201800\n",
            "In folder \"4_PAR\":\n",
            "Number of datasets = 36\n",
            "Number of unique months = 7\n",
            "\n",
            "Data length: 2403600\n",
            "In folder \"5_Kd\":\n",
            "Number of datasets = 36\n",
            "Number of unique months = 7\n",
            "\n",
            "Data length: 2403600\n",
            "In folder \"6_POC\":\n",
            "Number of datasets = 36\n",
            "Number of unique months = 7\n",
            "\n",
            "Data length: 2403600\n",
            "In folder \"7_nFLH\":\n",
            "Number of datasets = 36\n",
            "Number of unique months = 7\n",
            "\n",
            "Data length: 2403600\n",
            "In folder \"8_iPAR\":\n",
            "Number of datasets = 36\n",
            "Number of unique months = 7\n",
            "\n",
            "Data length: 2403600\n",
            "In folder \"91_CDOM\":\n",
            "Number of datasets = 36\n",
            "Number of unique months = 7\n",
            "\n",
            "Data length: 240240\n",
            "In folder \"92_SSHA\":\n",
            "Number of datasets = 217\n",
            "Number of unique months = 7\n",
            "\n",
            "Data length: 600600\n",
            "In folder \"93_NPP\":\n",
            "Number of datasets = 36\n",
            "Number of unique months = 7\n",
            "\n",
            "Data length: 58320\n",
            "In folder \"94_MIMOC_MLD\":\n",
            "Number of datasets = 7\n",
            "Number of unique months = 7\n",
            "\n",
            "Data length: 58320\n",
            "In folder \"95_MIMOC_TS\":\n",
            "Number of datasets = 7\n",
            "Number of unique months = 7\n",
            "\n",
            "~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
            "Execution time:\n",
            "944.7028 secs\n",
            "15.74505 mins\n",
            "0.26242 hrs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4PcOr3_3EFg"
      },
      "source": [
        "#-----------------------------------------------------------------------------\n",
        "# Pre-allocate extraction variables as a dict using Dask:\n",
        "variables = {'chl':dd.from_array(da.empty((dims[0,2],dims[0,3]))).compute(),\n",
        "             'PIC':dd.from_array(da.empty((dims[1,2],dims[1,3]))).compute(),\n",
        "             'SST':dd.from_array(da.empty((dims[2,2],dims[2,3]))).compute(),\n",
        "             'PAR':dd.from_array(da.empty((dims[3,2],dims[3,3]))).compute(),\n",
        "             'Kd':dd.from_array(da.empty((dims[4,2],dims[4,3]))).compute(),\n",
        "             'POC':dd.from_array(da.empty((dims[5,2],dims[5,3]))).compute(),\n",
        "             'FLH':dd.from_array(da.empty((dims[6,2],dims[6,3]))).compute(),\n",
        "             'iPAR':dd.from_array(da.empty((dims[7,2],dims[7,3]))).compute(),\n",
        "             'CDOM':dd.from_array(da.empty((dims[8,2],dims[8,3]))).compute(),\n",
        "             'SSHA':dd.from_array(da.empty((dims[9,2],dims[9,3]))).compute(),\n",
        "             'NPP':dd.from_array(da.empty((dims[10,2],dims[10,3]))).compute(),\n",
        "             'MLD':dd.from_array(da.empty((dims[11,2],dims[11,3]))).compute(),\n",
        "             'SAL':dd.from_array(da.empty((dims[12,2],dims[12,3]))).compute()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oyoQ0cZV3Jgn",
        "outputId": "b969c6c6-9363-470f-ee52-a5bd8d5d612e"
      },
      "source": [
        "#-----------------------------------------------------------------------------\n",
        "print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
        "print('Beginning extraction loop...')\n",
        "start = timeit.default_timer() # start the clock\n",
        "for count, (k, folder) in enumerate(zip(variables, folders)):\n",
        "    print('\\nAccessing ' + '\"' + folder + '\"' + ' folder...')\n",
        "    print()\n",
        "    path = os.path.join(directory, folder)\n",
        "    if count>=11: # My PC is RAM limited so this line bypasses looping at higher res (0.25x0.25) below to extract one variable at a time, saving memory space; if computing resources are not limited, comment out to extract all variables at once\n",
        "      # if count == 0 and k==0: # My PC is RAM limited so this line bypasses looping at higher res (0.25x0.25) below to extract one variable at a time, saving memory space; if computing resources are not limited, comment out to extract all variables at once\n",
        "      for subdir, dirs, files in os.walk(path):\n",
        "          for n, file in enumerate(files):\n",
        "              filepath = subdir + os.sep + file\n",
        "              # Print status: starting iteration\n",
        "              print(folder + ' (' + str(count+1) + ' of ' + str(np.size(folders)) + ' folders)' + ': Extracting... ' + str(n+1) +' of ' + str(np.size(files)))\n",
        "              #-----------------------------------------------------------------\n",
        "              # Extract data\n",
        "              if count < 11:\n",
        "                  if count < 9:\n",
        "                      # Open netCDF file\n",
        "                      vars_ = xr.open_dataset(filepath)\n",
        "                      if count == 0:\n",
        "                          # Chl:\n",
        "                          data = vars_.chlor_a.values\n",
        "                      if count == 1:\n",
        "                          # PIC:\n",
        "                          data = vars_.pic.values\n",
        "                      if count == 2:\n",
        "                          # SST:\n",
        "                          data = vars_.sst.values\n",
        "                      if count == 3:\n",
        "                          # PAR:\n",
        "                          data = vars_.par.values\n",
        "                      if count == 4:\n",
        "                          # Kd (490 nm)\n",
        "                          data = vars_.Kd_490.values\n",
        "                      if count == 5:\n",
        "                          # POC\n",
        "                          data = vars_.poc.values\n",
        "                      if count == 6:\n",
        "                          # FLH\n",
        "                          data = vars_.nflh.values\n",
        "                      if count == 7:\n",
        "                          # iPAR\n",
        "                          data = vars_.ipar.values\n",
        "                      if count == 8:\n",
        "                          # CDOM\n",
        "                          data = vars_.adg_443_giop.values\n",
        "                      lat = vars_.coords['lat'].values\n",
        "                      lon = vars_.coords['lon'].values\n",
        "                      # time = pd.to_datetime(vars_.time_coverage_start)+datetime.timedelta(days=2) # this adds 2 days to correct for SeaWiFS files starting on the last day of previous month\n",
        "                      time = int((pd.to_datetime(vars_.time_coverage_start)+datetime.timedelta(days=2)).strftime('%m')) # this adds 2 days to correct for SeaWiFS files starting on the last day of previous month\n",
        "                  if count == 9:\n",
        "                      # SSHA\n",
        "                      # Open netCDF file\n",
        "                      vars_ = xr.open_dataset(filepath)\n",
        "                      data = vars_.SLA.values[0,:,:].T # transpose to arrange as lat x lon\n",
        "                      lat = vars_.coords['Latitude'].values\n",
        "                      lon = vars_.coords['Longitude'].values\n",
        "                      lon = pd.Series(lon).where(lon<180, lon-360).values # convert to W/E coords\n",
        "                      # time = pd.to_datetime(vars_.coords['Time'].values)\n",
        "                      time = int(pd.to_datetime(vars_.coords['Time'].values).strftime('%m').values)\n",
        "                  if count == 10:\n",
        "                      # NPP\n",
        "                      vars_ = SD(filepath, SDC.READ)\n",
        "                      data = np.flipud(vars_.select('npp').get())\n",
        "                      ndeg = 180/1080\n",
        "                      lat = np.arange(-90,90,ndeg)\n",
        "                      lon = np.arange(-180,180,ndeg)\n",
        "                      # extract date from julian day in filename:\n",
        "                      julian_date = filepath.split(\".\")[1]\n",
        "                      # time = pd.to_datetime(datetime.datetime.strptime(julian_date[2:], '%y%j').date()) #strftime requires year to be cropped to last 2 digits\n",
        "                      time = int(pd.to_datetime(datetime.datetime.strptime(julian_date[2:], '%y%j').date()).strftime('%m')) #strftime requires year to be cropped to last 2 digits\n",
        "                  #-----------------------------------------------------------------\n",
        "                  # case 1: match pixels to high resolution satellite data\n",
        "                  if lon[1]-lon[0] >= grid: #i.e. if upsampling to finer grid size\n",
        "                      # Regrid data and interpolate though:\n",
        "                      # First find the indices to index by lat/lon\n",
        "                      latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "                      loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "\n",
        "                      # Restrict the data to the specified lat/lons\n",
        "                      data_indexed = data[latinds[:,None], loninds[None,:]][:,:,0]\n",
        "\n",
        "                      # Create data matrix with coordinates\n",
        "                      data_mat = pd.DataFrame(data_indexed)\n",
        "                      data_mat.columns = lon[loninds].flatten()\n",
        "                      data_mat.index = lat[latinds].flatten()\n",
        "                      #sort the data so that columns are ascending (matches 'new_shape' below)\n",
        "                      data_mat = data_mat.reindex(sorted(data_mat.columns), axis=1) # sort columns in ascending order\n",
        "\n",
        "                      # Create a new matrix with new gridded coordinates\n",
        "                      lat_new = np.arange(min_lat, max_lat+grid, grid)\n",
        "                      lon_new = np.arange(min_lon, max_lon+grid, grid)\n",
        "                      new_shape = pd.DataFrame(np.ones((lat_new.shape[0],lon_new.shape[0])))\n",
        "                      new_shape.columns=lon_new\n",
        "                      new_shape.index=lat_new\n",
        "\n",
        "                      # Now reindex data to new coordinates - add in NaNs to interpolate through\n",
        "                      # important: find corresponding nearest value in new index/columns to replace data coords by\n",
        "                      new_idx = [new_shape.index[abs(new_shape.index.values-i).argmin()] for i in data_mat.index]\n",
        "                      new_cols = [new_shape.columns[abs(new_shape.columns.values-i).argmin()] for i in data_mat.columns]\n",
        "                      # now rename idx/cols with new values in the data matrix (this allows the reindex_like function to properly map and insert nans below)\n",
        "                      data_mat.set_axis(new_idx, axis=0, inplace=True)\n",
        "                      data_mat.set_axis(new_cols, axis=1, inplace=True)\n",
        "\n",
        "                      data_proc = pd.DataFrame(data_mat.reindex_like(new_shape).stack(dropna=False))\n",
        "                      data_proc = data_proc.rename(columns={0:'data'})\n",
        "                      data_proc.index = data_proc.index.set_names(['latbins','lonbins'])\n",
        "                      data_proc['datetime'] = np.tile(time,data_proc.shape[0])\n",
        "                      data_proc.reset_index(inplace=True)\n",
        "                      data_proc.set_index(['datetime','latbins','lonbins'], inplace=True)\n",
        "                      # data_proc.reset_index(inplace=True)\n",
        "                      \n",
        "                  #-----------------------------------------------------------------\n",
        "                  # case 2: downsample to courser grid\n",
        "                  if lon[1]-lon[0] < grid: # i.e. if interpolating to courser grid\n",
        "                      # Bin the data\n",
        "                      \n",
        "                      # First find the indices to index by lat/lon\n",
        "                      latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "                      loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "                      \n",
        "                      # Restrict the data to the NE Pacific lat/lons\n",
        "                      data_indexed = data[latinds[:,None], loninds[None,:]][:,:,0]\n",
        "                      \n",
        "                      # Convert time/lats/lons into repeating matrices to match data dimensions\n",
        "                      lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
        "                      lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
        "                      time_mat = np.tile(time, len(np.ravel(lat_mat)))\n",
        "                      \n",
        "                      # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
        "                      # Need these as pandas dataframes to using binning scheme:\n",
        "                      d = {'datetime':time_mat,'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(data_indexed)}\n",
        "                      data_long = pd.DataFrame(data=d)\n",
        "                      \n",
        "                      # Bin data as averages across gridded spatial bins:\n",
        "                      to_bin = lambda x: np.round(x / grid) * grid\n",
        "                      data_long['latbins'] = data_long.lat.map(to_bin)\n",
        "                      data_long['lonbins'] = data_long.lon.map(to_bin)\n",
        "                      data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
        "                      \n",
        "                      # Rename binned columns + drop mean lat/lons:\n",
        "                      data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
        "                      # data_proc.reset_index(inplace=True) # remove index specification on columns\n",
        "              #-----------------------------------------------------------------\n",
        "              # Lets extract MIMOC data...\n",
        "              if count >= 11:\n",
        "                  if count == 11:\n",
        "                      # Open netCDF file\n",
        "                      vars_ = xr.open_dataset(filepath)\n",
        "                      data = vars_.DEPTH_MIXED_LAYER.values\n",
        "                  if count == 12:\n",
        "                      # Open netCDF file\n",
        "                      vars_ = xr.open_dataset(filepath)\n",
        "                      data = vars_.SALINITY[0,:,:].values # select values at surface layer (0 m)\n",
        "                  lat = vars_.LATITUDE.values\n",
        "                  lon = vars_.LONGITUDE.values\n",
        "                  lon = pd.Series(lon).where(lon<180, lon-360).values # convert to W/E coords\n",
        "                  julian_date = file.split(\"month\")[1].split('.nc')[0]\n",
        "                  time = int(julian_date) #strftime requires year to be cropped to last 2 digits\n",
        "                  #-----------------------------------------------------------------\n",
        "                  # case 1: match pixels to high resolution satellite data\n",
        "                  if lon[1]-lon[0] >= grid: #i.e. if upsampling to finer grid size\n",
        "                      # Regrid data and interpolate though:\n",
        "                      # First find the indices to index by lat/lon\n",
        "                      latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "                      loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "\n",
        "                      # Restrict the data to the specified lat/lons\n",
        "                      data_indexed = data[latinds[:,None], loninds[None,:]][:,:,0]\n",
        "\n",
        "                      # Create data matrix with coordinates\n",
        "                      data_mat = pd.DataFrame(data_indexed)\n",
        "                      data_mat.columns = lon[loninds].flatten()\n",
        "                      data_mat.index = lat[latinds].flatten()\n",
        "                      #sort the data so that columns are ascending (matches 'new_shape' below)\n",
        "                      data_mat = data_mat.reindex(sorted(data_mat.columns), axis=1) # sort columns in ascending order\n",
        "\n",
        "                      # Create a new matrix with new gridded coordinates\n",
        "                      lat_new = np.arange(min_lat, max_lat+grid, grid)\n",
        "                      lon_new = np.arange(min_lon, max_lon+grid, grid)\n",
        "                      new_shape = pd.DataFrame(np.ones((lat_new.shape[0],lon_new.shape[0])))\n",
        "                      new_shape.columns=lon_new\n",
        "                      new_shape.index=lat_new\n",
        "\n",
        "                      # Now reindex data to new coordinates - add in NaNs to interpolate through\n",
        "                      # important: find corresponding nearest value in new index/columns to replace data coords by\n",
        "                      new_idx = [new_shape.index[abs(new_shape.index.values-i).argmin()] for i in data_mat.index]\n",
        "                      new_cols = [new_shape.columns[abs(new_shape.columns.values-i).argmin()] for i in data_mat.columns]\n",
        "                      # now rename idx/cols with new values in the data matrix (this allows the reindex_like function to properly map and insert nans below)\n",
        "                      data_mat.set_axis(new_idx, axis=0, inplace=True)\n",
        "                      data_mat.set_axis(new_cols, axis=1, inplace=True)\n",
        "\n",
        "                      data_proc = pd.DataFrame(data_mat.reindex_like(new_shape).stack(dropna=False))\n",
        "                      data_proc = data_proc.rename(columns={0:'data'})\n",
        "                      data_proc.index = data_proc.index.set_names(['latbins','lonbins'])\n",
        "                      data_proc['datetime'] = np.tile(time,data_proc.shape[0])\n",
        "                      data_proc.reset_index(inplace=True)\n",
        "                      data_proc.set_index(['datetime','latbins','lonbins'], inplace=True)\n",
        "                      # data_proc.reset_index(inplace=True)\n",
        "                      \n",
        "                  #-----------------------------------------------------------------\n",
        "                  # case 2: downsample to courser grid\n",
        "                  if lon[1]-lon[0] < grid: # i.e. if interpolating to courser grid\n",
        "                      # Bin the data\n",
        "                      \n",
        "                      # First find the indices to index by lat/lon\n",
        "                      latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "                      loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "                      \n",
        "                      # Restrict the data to the NE Pacific lat/lons\n",
        "                      data_indexed = data[latinds[:,None], loninds[None,:]][:,:,0]\n",
        "                      \n",
        "                      # Convert time/lats/lons into repeating matrices to match data dimensions\n",
        "                      lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
        "                      lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
        "                      time_mat = np.tile(time, len(np.ravel(lat_mat)))\n",
        "                      \n",
        "                      # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
        "                      # Need these as pandas dataframes to using binning scheme:\n",
        "                      d = {'datetime':time_mat,'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(data_indexed)}\n",
        "                      data_long = pd.DataFrame(data=d)\n",
        "                      \n",
        "                      # Bin data as averages across gridded spatial bins:\n",
        "                      to_bin = lambda x: np.round(x / grid) * grid\n",
        "                      data_long['latbins'] = data_long.lat.map(to_bin)\n",
        "                      data_long['lonbins'] = data_long.lon.map(to_bin)\n",
        "                      data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
        "                      \n",
        "                      # Rename binned columns + drop mean lat/lons:\n",
        "                      data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
        "                      # data_proc.reset_index(inplace=True) # remove index specification on columns\n",
        "              if count <= 9:\n",
        "                # close xarray netCDF file connection \n",
        "                vars_.close()            \n",
        "              # Save to output array\n",
        "              if n == 0: # for first iteration, define pre-allocated variable in dict as new binned data\n",
        "                  variables[k] = data_proc\n",
        "              else: # for subsequent iterations, add to existing dict variable by \"stacking\" new binned datasets row-wise\n",
        "                  # data_proc = data_proc.set_index(['datetime','latbins','lonbins'])\n",
        "                  variables[k] = pd.concat([variables[k],data_proc]).groupby(['datetime','latbins','lonbins']).mean()\n",
        "              \n",
        "              # Print status: completed iteration\n",
        "              print(folder + ' (' + str(count+1) + ' of ' + str(np.size(folders)) + ' folders)' + ': Completed ' + str(n+1) + ' of ' + str(np.size(files)))\n",
        "      print('\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
        "      print('Ta-da! ' + str(count+1) + ' of ' + str(np.size(folders)) + ' datasets processed')\n",
        "end = timeit.default_timer() # stop the clock\n",
        "print('\\n~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~')\n",
        "print('Execution time:') # output computation times:\n",
        "print(str(round(end-start,5)),'secs')\n",
        "print(str(round((end-start)/60,5)),'mins')\n",
        "print(str(round((end-start)/3600,5)),'hrs')\n",
        "del vars_, data_long, data, data_indexed, data_proc, time_mat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Beginning extraction loop...\n",
            "\n",
            "Accessing \"1_Chl\" folder...\n",
            "\n",
            "\n",
            "Accessing \"2_PIC\" folder...\n",
            "\n",
            "\n",
            "Accessing \"3_SST\" folder...\n",
            "\n",
            "\n",
            "Accessing \"4_PAR\" folder...\n",
            "\n",
            "\n",
            "Accessing \"5_Kd\" folder...\n",
            "\n",
            "\n",
            "Accessing \"6_POC\" folder...\n",
            "\n",
            "\n",
            "Accessing \"7_nFLH\" folder...\n",
            "\n",
            "\n",
            "Accessing \"8_iPAR\" folder...\n",
            "\n",
            "\n",
            "Accessing \"91_CDOM\" folder...\n",
            "\n",
            "\n",
            "Accessing \"92_SSHA\" folder...\n",
            "\n",
            "\n",
            "Accessing \"93_NPP\" folder...\n",
            "\n",
            "\n",
            "Accessing \"94_MIMOC_MLD\" folder...\n",
            "\n",
            "94_MIMOC_MLD (12 of 13 folders): Extracting... 1 of 7\n",
            "94_MIMOC_MLD (12 of 13 folders): Completed 1 of 7\n",
            "94_MIMOC_MLD (12 of 13 folders): Extracting... 2 of 7\n",
            "94_MIMOC_MLD (12 of 13 folders): Completed 2 of 7\n",
            "94_MIMOC_MLD (12 of 13 folders): Extracting... 3 of 7\n",
            "94_MIMOC_MLD (12 of 13 folders): Completed 3 of 7\n",
            "94_MIMOC_MLD (12 of 13 folders): Extracting... 4 of 7\n",
            "94_MIMOC_MLD (12 of 13 folders): Completed 4 of 7\n",
            "94_MIMOC_MLD (12 of 13 folders): Extracting... 5 of 7\n",
            "94_MIMOC_MLD (12 of 13 folders): Completed 5 of 7\n",
            "94_MIMOC_MLD (12 of 13 folders): Extracting... 6 of 7\n",
            "94_MIMOC_MLD (12 of 13 folders): Completed 6 of 7\n",
            "94_MIMOC_MLD (12 of 13 folders): Extracting... 7 of 7\n",
            "94_MIMOC_MLD (12 of 13 folders): Completed 7 of 7\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Ta-da! 12 of 13 datasets processed\n",
            "\n",
            "Accessing \"95_MIMOC_TS\" folder...\n",
            "\n",
            "95_MIMOC_TS (13 of 13 folders): Extracting... 1 of 7\n",
            "95_MIMOC_TS (13 of 13 folders): Completed 1 of 7\n",
            "95_MIMOC_TS (13 of 13 folders): Extracting... 2 of 7\n",
            "95_MIMOC_TS (13 of 13 folders): Completed 2 of 7\n",
            "95_MIMOC_TS (13 of 13 folders): Extracting... 3 of 7\n",
            "95_MIMOC_TS (13 of 13 folders): Completed 3 of 7\n",
            "95_MIMOC_TS (13 of 13 folders): Extracting... 4 of 7\n",
            "95_MIMOC_TS (13 of 13 folders): Completed 4 of 7\n",
            "95_MIMOC_TS (13 of 13 folders): Extracting... 5 of 7\n",
            "95_MIMOC_TS (13 of 13 folders): Completed 5 of 7\n",
            "95_MIMOC_TS (13 of 13 folders): Extracting... 6 of 7\n",
            "95_MIMOC_TS (13 of 13 folders): Completed 6 of 7\n",
            "95_MIMOC_TS (13 of 13 folders): Extracting... 7 of 7\n",
            "95_MIMOC_TS (13 of 13 folders): Completed 7 of 7\n",
            "\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "Ta-da! 13 of 13 datasets processed\n",
            "\n",
            "~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
            "Execution time:\n",
            "11.42618 secs\n",
            "0.19044 mins\n",
            "0.00317 hrs\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ea23794e7ee1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mins'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3600\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'hrs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mvars_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_indexed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_proc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_mat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'time_mat' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "wZQHJTU8yRZg",
        "outputId": "05f23bf7-2c8b-425a-c358-380f2311ffac"
      },
      "source": [
        "# Download processed files\n",
        "from google.colab import files\n",
        "\n",
        "for i, var_ in enumerate(variables):\n",
        "  # if var_ == 'POC':\n",
        "  if i >= 11:\n",
        "    variables[var_].to_csv(write_to+str(var_)+'_'+str(round(grid,3))+'_deg.csv')\n",
        "    files.download(write_to+str(var_)+'_'+str(round(grid,3))+'_deg.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_71869fef-6112-448c-b52b-ac89a38822d9\", \"SO_DMS_MLD_0.18_deg.csv\", 163898647)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_8eae556c-d05a-4f6e-8e11-9c572fb9a0f6\", \"SO_DMS_SAL_0.18_deg.csv\", 163910857)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMdGBjLgSdcx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "a5d361cf-dc32-4fe9-84fc-27ef60d72d4a"
      },
      "source": [
        "#%% Extracting COPERNICUS wind field data (singular netCDF; 0.25x0.25)\n",
        "\n",
        "print('\\nExtracting Copernicus wind field data...')\n",
        "print('\\nLoading data...')\n",
        "# Pre-allocate dask array to store new binned data to\n",
        "wind = dd.from_array(da.empty((1,6)))\n",
        "# Load up wind data file (Copernicus timeseries data is downloaded into a single netCDF)\n",
        "windfile = xr.open_dataset('CERSAT-GLO-REP_WIND_L4-OBS_FULL_TIME_SERIE_1607246388558.nc')\n",
        "\n",
        "# Define variables\n",
        "# shape is (time,depth,lat,lon)\n",
        "lat = windfile.coords['latitude'].values\n",
        "lon = windfile.coords['longitude'].values\n",
        "Us = windfile.eastward_wind.values[:,0,:,:]\n",
        "Vs = windfile.northward_wind.values[:,0,:,:]\n",
        "wspd = windfile.wind_speed.values[:,0,:,:]\n",
        "timerange = pd.to_datetime(windfile.time.values,utc=True).strftime('%Y%m').to_series().reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Define indices\n",
        "latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "if which_dataset == 'DMS':\n",
        "  timeinds = ['200112', '200201', '200202', '200301', '200302', '200303',\n",
        "              '200310', '200311', '200401', '200402', '200412', '200501',\n",
        "              '200502', '200510', '200511', '200512', '200601', '200602',\n",
        "              '200611', '200612', '200711', '200712', '200801', '200802',\n",
        "              '200803', '200804', '200812', '200901', '200902', '201012',\n",
        "              '201202', '201203', '201311', '201312', '201402', '201403'][20:] # index from 2007 onwards to match data\n",
        "else: # if Fe\n",
        "  timeinds = ['200707', '200708', '200709', '200711', '200712', '200802',\n",
        "              '200803', '200804', '200912', '201005', '201006', '201007',\n",
        "              '201010', '201011', '201103', '201105', '201208', '201209',\n",
        "              '201210']\n",
        "\n",
        "idx = np.arange(0,timerange.size,1)\n",
        "times = pd.DataFrame(idx,index=timerange)\n",
        "timepts = times.loc[timeinds].values\n",
        "\n",
        "\n",
        "print('\\nBegin extraction loop...')\n",
        "# Loop to bin/reshape\n",
        "for n,pt in enumerate(timepts[:,0]):\n",
        "    print(str(n+1)+' of '+str(np.size(timepts[:,0]))+' iterations...')\n",
        "    #===========================================================================\n",
        "    #                             Wind speed\n",
        "    #===========================================================================\n",
        "    # case 1: match pixels to high resolution satellite data\n",
        "    if lon[1]-lon[0] >= grid: #i.e. if upsampling to finer grid size\n",
        "        # Regrid data and interpolate though:\n",
        "        # First find the indices to index by lat/lon\n",
        "        latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "        loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "\n",
        "        # Restrict the data to the specified lat/lons\n",
        "        data_indexed = wspd[pt,:,:][latinds[:,None], loninds[None,:]][:,:,0]\n",
        "\n",
        "        # Create data matrix with coordinates\n",
        "        data_mat = pd.DataFrame(data_indexed)\n",
        "        data_mat.columns = lon[loninds].flatten()\n",
        "        data_mat.index = lat[latinds].flatten()\n",
        "        #sort the data so that columns are ascending (matches 'new_shape' below)\n",
        "        data_mat = data_mat.reindex(sorted(data_mat.columns), axis=1) # sort columns in ascending order\n",
        "\n",
        "        # Create a new matrix with new gridded coordinates\n",
        "        lat_new = np.arange(min_lat, max_lat+grid, grid)\n",
        "        lon_new = np.arange(min_lon, max_lon+grid, grid)\n",
        "        new_shape = pd.DataFrame(np.ones((lat_new.shape[0],lon_new.shape[0])))\n",
        "        new_shape.columns=lon_new\n",
        "        new_shape.index=lat_new\n",
        "\n",
        "        # Now reindex data to new coordinates - add in NaNs to interpolate through\n",
        "        # important: find corresponding nearest value in new index/columns to replace data coords by\n",
        "        new_idx = [new_shape.index[abs(new_shape.index.values-i).argmin()] for i in data_mat.index]\n",
        "        new_cols = [new_shape.columns[abs(new_shape.columns.values-i).argmin()] for i in data_mat.columns]\n",
        "        # now rename idx/cols with new values in the data matrix (this allows the reindex_like function to properly map and insert nans below)\n",
        "        data_mat.set_axis(new_idx, axis=0, inplace=True)\n",
        "        data_mat.set_axis(new_cols, axis=1, inplace=True)\n",
        "\n",
        "        data_proc = pd.DataFrame(data_mat.reindex_like(new_shape).stack(dropna=False))\n",
        "        data_proc = data_proc.rename(columns={0:'data'})\n",
        "        data_proc.index = data_proc.index.set_names(['latbins','lonbins'])\n",
        "        data_proc['datetime'] = np.tile(int(pd.to_datetime(datetime.datetime.strptime(timeinds[n], '%Y%m').strftime('%Y-%m')).strftime('%m')),data_proc.shape[0])\n",
        "        data_proc.reset_index(inplace=True)\n",
        "        data_proc.set_index(['datetime','latbins','lonbins'], inplace=True)\n",
        "        # data_proc.reset_index(inplace=True)\n",
        "        \n",
        "    #-----------------------------------------------------------------\n",
        "    # case 2: downsample to courser grid\n",
        "    if lon[1]-lon[0] < grid: # i.e. if interpolating to courser grid\n",
        "        # Bin the data\n",
        "        \n",
        "        # First find the indices to index by lat/lon\n",
        "        latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "        loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "        \n",
        "        # Restrict the data to the NE Pacific lat/lons\n",
        "        data_indexed = wspd[pt,:,:][latinds[:,None], loninds[None,:]][:,:,0]\n",
        "        \n",
        "        # Convert time/lats/lons into repeating matrices to match data dimensions\n",
        "        lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
        "        lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
        "        time_mat = np.tile(int(pd.to_datetime(datetime.datetime.strptime(timeinds[n], '%Y%m').strftime('%Y-%m')).strftime('%m')), len(np.ravel(lat_mat)))\n",
        "        \n",
        "        # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
        "        # Need these as pandas dataframes to using binning scheme:\n",
        "        d = {'datetime':time_mat,'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(data_indexed)}\n",
        "        data_long = pd.DataFrame(data=d)\n",
        "        \n",
        "        # Bin data as averages across gridded spatial bins:\n",
        "        to_bin = lambda x: np.round(x / grid) * grid\n",
        "        data_long['latbins'] = data_long.lat.map(to_bin)\n",
        "        data_long['lonbins'] = data_long.lon.map(to_bin)\n",
        "        data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
        "        \n",
        "        # Rename binned columns + drop mean lat/lons:\n",
        "        data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
        "    # Save to output array\n",
        "    if n == 0: # for first iteration, define pre-allocated variable in dict as new binned data\n",
        "        wind = data_proc\n",
        "    else: # for subsequent iterations, add to existing dict variable by \"stacking\" new binned datasets row-wise\n",
        "        wind = pd.concat([wind,data_proc]).groupby(['datetime','latbins','lonbins']).mean()\n",
        "    #===========================================================================\n",
        "    #                             U component\n",
        "    #===========================================================================\n",
        "    # case 1: match pixels to high resolution satellite data\n",
        "    if lon[1]-lon[0] >= grid: #i.e. if upsampling to finer grid size\n",
        "        # Regrid data and interpolate though:\n",
        "        # First find the indices to index by lat/lon\n",
        "        latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "        loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "\n",
        "        # Restrict the data to the specified lat/lons\n",
        "        data_indexed = Us[pt,:,:][latinds[:,None], loninds[None,:]][:,:,0]\n",
        "\n",
        "        # Create data matrix with coordinates\n",
        "        data_mat = pd.DataFrame(data_indexed)\n",
        "        data_mat.columns = lon[loninds].flatten()\n",
        "        data_mat.index = lat[latinds].flatten()\n",
        "        #sort the data so that columns are ascending (matches 'new_shape' below)\n",
        "        data_mat = data_mat.reindex(sorted(data_mat.columns), axis=1) # sort columns in ascending order\n",
        "\n",
        "        # Create a new matrix with new gridded coordinates\n",
        "        lat_new = np.arange(min_lat, max_lat+grid, grid)\n",
        "        lon_new = np.arange(min_lon, max_lon+grid, grid)\n",
        "        new_shape = pd.DataFrame(np.ones((lat_new.shape[0],lon_new.shape[0])))\n",
        "        new_shape.columns=lon_new\n",
        "        new_shape.index=lat_new\n",
        "\n",
        "        # Now reindex data to new coordinates - add in NaNs to interpolate through\n",
        "        # important: find corresponding nearest value in new index/columns to replace data coords by\n",
        "        new_idx = [new_shape.index[abs(new_shape.index.values-i).argmin()] for i in data_mat.index]\n",
        "        new_cols = [new_shape.columns[abs(new_shape.columns.values-i).argmin()] for i in data_mat.columns]\n",
        "        # now rename idx/cols with new values in the data matrix (this allows the reindex_like function to properly map and insert nans below)\n",
        "        data_mat.set_axis(new_idx, axis=0, inplace=True)\n",
        "        data_mat.set_axis(new_cols, axis=1, inplace=True)\n",
        "\n",
        "        data_proc = pd.DataFrame(data_mat.reindex_like(new_shape).stack(dropna=False))\n",
        "        data_proc = data_proc.rename(columns={0:'data'})\n",
        "        data_proc.index = data_proc.index.set_names(['latbins','lonbins'])\n",
        "        data_proc['datetime'] = np.tile(int(pd.to_datetime(datetime.datetime.strptime(timeinds[n], '%Y%m').strftime('%Y-%m')).strftime('%m')),data_proc.shape[0])\n",
        "        data_proc.reset_index(inplace=True)\n",
        "        data_proc.set_index(['datetime','latbins','lonbins'], inplace=True)\n",
        "        # data_proc.reset_index(inplace=True)\n",
        "        \n",
        "    #-----------------------------------------------------------------\n",
        "    # case 2: downsample to courser grid\n",
        "    if lon[1]-lon[0] < grid: # i.e. if interpolating to courser grid\n",
        "        # Bin the data\n",
        "        \n",
        "        # First find the indices to index by lat/lon\n",
        "        latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "        loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "        \n",
        "        # Restrict the data to the NE Pacific lat/lons\n",
        "        data_indexed = Us[pt,:,:][latinds[:,None], loninds[None,:]][:,:,0]\n",
        "        \n",
        "        # Convert time/lats/lons into repeating matrices to match data dimensions\n",
        "        lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
        "        lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
        "        time_mat = np.tile(int(pd.to_datetime(datetime.datetime.strptime(timeinds[n], '%Y%m').strftime('%Y-%m')).strftime('%m')), len(np.ravel(lat_mat)))\n",
        "        \n",
        "        # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
        "        # Need these as pandas dataframes to using binning scheme:\n",
        "        d = {'datetime':time_mat,'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(data_indexed)}\n",
        "        data_long = pd.DataFrame(data=d)\n",
        "        \n",
        "        # Bin data as averages across gridded spatial bins:\n",
        "        to_bin = lambda x: np.round(x / grid) * grid\n",
        "        data_long['latbins'] = data_long.lat.map(to_bin)\n",
        "        data_long['lonbins'] = data_long.lon.map(to_bin)\n",
        "        data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
        "        \n",
        "        # Rename binned columns + drop mean lat/lons:\n",
        "        data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
        "    # Save to output array\n",
        "    if n == 0: # for first iteration, define pre-allocated variable in dict as new binned data\n",
        "        U = data_proc\n",
        "    else: # for subsequent iterations, add to existing dict variable by \"stacking\" new binned datasets row-wise\n",
        "        U = pd.concat([U,data_proc]).groupby(['datetime','latbins','lonbins']).mean()\n",
        "    #===========================================================================\n",
        "    #                             V component\n",
        "    #===========================================================================\n",
        "    # case 1: match pixels to high resolution satellite data\n",
        "    if lon[1]-lon[0] >= grid: #i.e. if upsampling to finer grid size\n",
        "        # Regrid data and interpolate though:\n",
        "        # First find the indices to index by lat/lon\n",
        "        latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "        loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "\n",
        "        # Restrict the data to the specified lat/lons\n",
        "        data_indexed = Vs[pt,:,:][latinds[:,None], loninds[None,:]][:,:,0]\n",
        "\n",
        "        # Create data matrix with coordinates\n",
        "        data_mat = pd.DataFrame(data_indexed)\n",
        "        data_mat.columns = lon[loninds].flatten()\n",
        "        data_mat.index = lat[latinds].flatten()\n",
        "        #sort the data so that columns are ascending (matches 'new_shape' below)\n",
        "        data_mat = data_mat.reindex(sorted(data_mat.columns), axis=1) # sort columns in ascending order\n",
        "\n",
        "        # Create a new matrix with new gridded coordinates\n",
        "        lat_new = np.arange(min_lat, max_lat+grid, grid)\n",
        "        lon_new = np.arange(min_lon, max_lon+grid, grid)\n",
        "        new_shape = pd.DataFrame(np.ones((lat_new.shape[0],lon_new.shape[0])))\n",
        "        new_shape.columns=lon_new\n",
        "        new_shape.index=lat_new\n",
        "\n",
        "        # Now reindex data to new coordinates - add in NaNs to interpolate through\n",
        "        # important: find corresponding nearest value in new index/columns to replace data coords by\n",
        "        new_idx = [new_shape.index[abs(new_shape.index.values-i).argmin()] for i in data_mat.index]\n",
        "        new_cols = [new_shape.columns[abs(new_shape.columns.values-i).argmin()] for i in data_mat.columns]\n",
        "        # now rename idx/cols with new values in the data matrix (this allows the reindex_like function to properly map and insert nans below)\n",
        "        data_mat.set_axis(new_idx, axis=0, inplace=True)\n",
        "        data_mat.set_axis(new_cols, axis=1, inplace=True)\n",
        "\n",
        "        data_proc = pd.DataFrame(data_mat.reindex_like(new_shape).stack(dropna=False))\n",
        "        data_proc = data_proc.rename(columns={0:'data'})\n",
        "        data_proc.index = data_proc.index.set_names(['latbins','lonbins'])\n",
        "        data_proc['datetime'] = np.tile(int(pd.to_datetime(datetime.datetime.strptime(timeinds[n], '%Y%m').strftime('%Y-%m')).strftime('%m')),data_proc.shape[0])\n",
        "        data_proc.reset_index(inplace=True)\n",
        "        data_proc.set_index(['datetime','latbins','lonbins'], inplace=True)\n",
        "        # data_proc.reset_index(inplace=True)\n",
        "        \n",
        "    #-----------------------------------------------------------------\n",
        "    # case 2: downsample to courser grid\n",
        "    if lon[1]-lon[0] < grid: # i.e. if interpolating to courser grid\n",
        "        # Bin the data\n",
        "        \n",
        "        # First find the indices to index by lat/lon\n",
        "        latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "        loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "        \n",
        "        # Restrict the data to the NE Pacific lat/lons\n",
        "        data_indexed = Vs[pt,:,:][latinds[:,None], loninds[None,:]][:,:,0]\n",
        "        \n",
        "        # Convert time/lats/lons into repeating matrices to match data dimensions\n",
        "        lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
        "        lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
        "        time_mat = np.tile(int(pd.to_datetime(datetime.datetime.strptime(timeinds[n], '%Y%m').strftime('%Y-%m')).strftime('%m')), len(np.ravel(lat_mat)))\n",
        "        \n",
        "        # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
        "        # Need these as pandas dataframes to using binning scheme:\n",
        "        d = {'datetime':time_mat,'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(data_indexed)}\n",
        "        data_long = pd.DataFrame(data=d)\n",
        "        \n",
        "        # Bin data as averages across gridded spatial bins:\n",
        "        to_bin = lambda x: np.round(x / grid) * grid\n",
        "        data_long['latbins'] = data_long.lat.map(to_bin)\n",
        "        data_long['lonbins'] = data_long.lon.map(to_bin)\n",
        "        data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
        "        \n",
        "        # Rename binned columns + drop mean lat/lons:\n",
        "        data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
        "    # Save to output array\n",
        "    if n == 0: # for first iteration, define pre-allocated variable in dict as new binned data\n",
        "        V = data_proc\n",
        "    else: # for subsequent iterations, add to existing dict variable by \"stacking\" new binned datasets row-wise\n",
        "        V = pd.concat([V,data_proc]).groupby(['datetime','latbins','lonbins']).mean()\n",
        "    print(str(n+1)+' of '+str(np.size(timepts[:,0]))+' iterations completed!')\n",
        "del windfile\n",
        "#===============================================================================\n",
        "# write data to file\n",
        "from google.colab import files\n",
        "wind.to_csv(write_to+'wind_copernicus'+'_'+str(round(grid,3))+'_deg.csv')\n",
        "U.to_csv(write_to+'U_copernicus'+'_'+str(round(grid,3))+'_deg.csv')\n",
        "V.to_csv(write_to+'V_copernicus'+'_'+str(round(grid,3))+'_deg.csv')\n",
        "files.download(write_to+'wind_copernicus'+'_'+str(round(grid,3))+'_deg.csv')\n",
        "files.download(write_to+'U_copernicus'+'_'+str(round(grid,3))+'_deg.csv')\n",
        "files.download(write_to+'V_copernicus'+'_'+str(round(grid,3))+'_deg.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting Copernicus wind field data...\n",
            "\n",
            "Loading data...\n",
            "\n",
            "Begin extraction loop...\n",
            "1 of 16 iterations...\n",
            "1 of 16 iterations completed!\n",
            "2 of 16 iterations...\n",
            "2 of 16 iterations completed!\n",
            "3 of 16 iterations...\n",
            "3 of 16 iterations completed!\n",
            "4 of 16 iterations...\n",
            "4 of 16 iterations completed!\n",
            "5 of 16 iterations...\n",
            "5 of 16 iterations completed!\n",
            "6 of 16 iterations...\n",
            "6 of 16 iterations completed!\n",
            "7 of 16 iterations...\n",
            "7 of 16 iterations completed!\n",
            "8 of 16 iterations...\n",
            "8 of 16 iterations completed!\n",
            "9 of 16 iterations...\n",
            "9 of 16 iterations completed!\n",
            "10 of 16 iterations...\n",
            "10 of 16 iterations completed!\n",
            "11 of 16 iterations...\n",
            "11 of 16 iterations completed!\n",
            "12 of 16 iterations...\n",
            "12 of 16 iterations completed!\n",
            "13 of 16 iterations...\n",
            "13 of 16 iterations completed!\n",
            "14 of 16 iterations...\n",
            "14 of 16 iterations completed!\n",
            "15 of 16 iterations...\n",
            "15 of 16 iterations completed!\n",
            "16 of 16 iterations...\n",
            "16 of 16 iterations completed!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_748bc187-252c-4c3d-a30e-9dedbae5ce7c\", \"SO_DMS_wind_copernicus_0.18_deg.csv\", 142920543)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_85b6efb2-0735-4f7d-9a24-51761bfbfd57\", \"SO_DMS_U_copernicus_0.18_deg.csv\", 143291024)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_c454cc1d-bac9-44d1-a3e4-51329a754af1\", \"SO_DMS_V_copernicus_0.18_deg.csv\", 143331787)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPc5eOwgo68Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c4cbf155-1bd9-4600-fe33-fb2af9f8da92"
      },
      "source": [
        "#%% Process CCMP wind speed data (multiple netcdfs, 0.25x0.25o)\n",
        "if which_dataset == 'DMS':\n",
        "  \n",
        "  wind_dir = 'CCMP_0.25_wind_speeds' # E: or C:, depending if using hard drive/thumb drive\n",
        "\n",
        "  files = sorted(os.listdir(wind_dir)) # calls specific datasets grouped in folders\n",
        "  # path = os.path.join(wind_dir, folder)\n",
        "\n",
        "  for subdir, dirs, files in os.walk(wind_dir):\n",
        "          for n, file in enumerate(files):\n",
        "            filepath = subdir + os.sep + file\n",
        "            #-------------------------------------------------------------------\n",
        "            # wind speed\n",
        "            vars_ = xr.open_dataset(filepath)\n",
        "            data = vars_.wspd.values[0,:,:]\n",
        "            lat = vars_.lat.values\n",
        "            lon = vars_.lon.values\n",
        "            lon = pd.Series(lon).where(lon<180, lon-360).values # convert to W/E coords\n",
        "            time = int(pd.to_datetime(vars_.time.values).strftime('%m').values)\n",
        "            if lon[1]-lon[0] >= grid: #i.e. if upsampling to finer grid size\n",
        "                # Regrid data and interpolate though:\n",
        "                # First find the indices to index by lat/lon\n",
        "                latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "                loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "\n",
        "                # Restrict the data to the specified lat/lons\n",
        "                data_indexed = data[latinds[:,None], loninds[None,:]][:,:,0]\n",
        "\n",
        "                # Create data matrix with coordinates\n",
        "                data_mat = pd.DataFrame(data_indexed)\n",
        "                data_mat.columns = lon[loninds].flatten()\n",
        "                data_mat.index = lat[latinds].flatten()\n",
        "                #sort the data so that columns are ascending (matches 'new_shape' below)\n",
        "                data_mat = data_mat.reindex(sorted(data_mat.columns), axis=1) # sort columns in ascending order\n",
        "\n",
        "                # Create a new matrix with new gridded coordinates\n",
        "                lat_new = np.arange(min_lat, max_lat+grid, grid)\n",
        "                lon_new = np.arange(min_lon, max_lon+grid, grid)\n",
        "                new_shape = pd.DataFrame(np.ones((lat_new.shape[0],lon_new.shape[0])))\n",
        "                new_shape.columns=lon_new\n",
        "                new_shape.index=lat_new\n",
        "\n",
        "                # Now reindex data to new coordinates - add in NaNs to interpolate through\n",
        "                # important: find corresponding nearest value in new index/columns to replace data coords by\n",
        "                new_idx = [new_shape.index[abs(new_shape.index.values-i).argmin()] for i in data_mat.index]\n",
        "                new_cols = [new_shape.columns[abs(new_shape.columns.values-i).argmin()] for i in data_mat.columns]\n",
        "                # now rename idx/cols with new values in the data matrix (this allows the reindex_like function to properly map and insert nans below)\n",
        "                data_mat.set_axis(new_idx, axis=0, inplace=True)\n",
        "                data_mat.set_axis(new_cols, axis=1, inplace=True)\n",
        "\n",
        "                data_proc = pd.DataFrame(data_mat.reindex_like(new_shape).stack(dropna=False))\n",
        "                data_proc = data_proc.rename(columns={0:'data'})\n",
        "                data_proc.index = data_proc.index.set_names(['latbins','lonbins'])\n",
        "                data_proc['datetime'] = np.tile(time,data_proc.shape[0])\n",
        "                data_proc.reset_index(inplace=True)\n",
        "                data_proc.set_index(['datetime','latbins','lonbins'], inplace=True)\n",
        "                # data_proc.reset_index(inplace=True)\n",
        "                \n",
        "            #-----------------------------------------------------------------\n",
        "            # case 2: downsample to courser grid\n",
        "            if lon[1]-lon[0] < grid: # i.e. if interpolating to courser grid\n",
        "                # Bin the data\n",
        "                \n",
        "                # First find the indices to index by lat/lon\n",
        "                latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "                loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "                \n",
        "                # Restrict the data to the NE Pacific lat/lons\n",
        "                data_indexed = data[latinds[:,None], loninds[None,:]][:,:,0]\n",
        "                \n",
        "                # Convert time/lats/lons into repeating matrices to match data dimensions\n",
        "                lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
        "                lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
        "                time_mat = np.tile(time, len(np.ravel(lat_mat)))\n",
        "                \n",
        "                # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
        "                # Need these as pandas dataframes to using binning scheme:\n",
        "                d = {'datetime':time_mat,'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(data_indexed)}\n",
        "                data_long = pd.DataFrame(data=d)\n",
        "                \n",
        "                # Bin data as averages across gridded spatial bins:\n",
        "                to_bin = lambda x: np.round(x / grid) * grid\n",
        "                data_long['latbins'] = data_long.lat.map(to_bin)\n",
        "                data_long['lonbins'] = data_long.lon.map(to_bin)\n",
        "                data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
        "                \n",
        "                # Rename binned columns + drop mean lat/lons:\n",
        "                data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
        "            # Save to output array\n",
        "            if n == 0: # for first iteration, define pre-allocated variable in dict as new binned data\n",
        "              wind = data_proc\n",
        "            else: # for subsequent iterations, add to existing dict variable by \"stacking\" new binned datasets row-wise\n",
        "              wind = pd.concat([wind,data_proc]).groupby(['datetime','latbins','lonbins']).mean()\n",
        "            #-------------------------------------------------------------------\n",
        "            # U vector\n",
        "            vars_ = xr.open_dataset(filepath)\n",
        "            data = vars_.uwnd.values[0,:,:]\n",
        "            lat = vars_.lat.values\n",
        "            lon = vars_.lon.values\n",
        "            lon = pd.Series(lon).where(lon<180, lon-360).values # convert to W/E coords\n",
        "            time = int(pd.to_datetime(vars_.time.values).strftime('%m').values)\n",
        "            if lon[1]-lon[0] >= grid: #i.e. if upsampling to finer grid size\n",
        "                # Regrid data and interpolate though:\n",
        "                # First find the indices to index by lat/lon\n",
        "                latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "                loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "\n",
        "                # Restrict the data to the specified lat/lons\n",
        "                data_indexed = data[latinds[:,None], loninds[None,:]][:,:,0]\n",
        "\n",
        "                # Create data matrix with coordinates\n",
        "                data_mat = pd.DataFrame(data_indexed)\n",
        "                data_mat.columns = lon[loninds].flatten()\n",
        "                data_mat.index = lat[latinds].flatten()\n",
        "                #sort the data so that columns are ascending (matches 'new_shape' below)\n",
        "                data_mat = data_mat.reindex(sorted(data_mat.columns), axis=1) # sort columns in ascending order\n",
        "\n",
        "                # Create a new matrix with new gridded coordinates\n",
        "                lat_new = np.arange(min_lat, max_lat+grid, grid)\n",
        "                lon_new = np.arange(min_lon, max_lon+grid, grid)\n",
        "                new_shape = pd.DataFrame(np.ones((lat_new.shape[0],lon_new.shape[0])))\n",
        "                new_shape.columns=lon_new\n",
        "                new_shape.index=lat_new\n",
        "\n",
        "                # Now reindex data to new coordinates - add in NaNs to interpolate through\n",
        "                # important: find corresponding nearest value in new index/columns to replace data coords by\n",
        "                new_idx = [new_shape.index[abs(new_shape.index.values-i).argmin()] for i in data_mat.index]\n",
        "                new_cols = [new_shape.columns[abs(new_shape.columns.values-i).argmin()] for i in data_mat.columns]\n",
        "                # now rename idx/cols with new values in the data matrix (this allows the reindex_like function to properly map and insert nans below)\n",
        "                data_mat.set_axis(new_idx, axis=0, inplace=True)\n",
        "                data_mat.set_axis(new_cols, axis=1, inplace=True)\n",
        "\n",
        "                data_proc = pd.DataFrame(data_mat.reindex_like(new_shape).stack(dropna=False))\n",
        "                data_proc = data_proc.rename(columns={0:'data'})\n",
        "                data_proc.index = data_proc.index.set_names(['latbins','lonbins'])\n",
        "                data_proc['datetime'] = np.tile(time,data_proc.shape[0])\n",
        "                data_proc.reset_index(inplace=True)\n",
        "                data_proc.set_index(['datetime','latbins','lonbins'], inplace=True)\n",
        "                # data_proc.reset_index(inplace=True)\n",
        "                \n",
        "            #-----------------------------------------------------------------\n",
        "            # case 2: downsample to courser grid\n",
        "            if lon[1]-lon[0] < grid: # i.e. if interpolating to courser grid\n",
        "                # Bin the data\n",
        "                \n",
        "                # First find the indices to index by lat/lon\n",
        "                latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "                loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "                \n",
        "                # Restrict the data to the NE Pacific lat/lons\n",
        "                data_indexed = data[latinds[:,None], loninds[None,:]][:,:,0]\n",
        "                \n",
        "                # Convert time/lats/lons into repeating matrices to match data dimensions\n",
        "                lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
        "                lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
        "                time_mat = np.tile(time, len(np.ravel(lat_mat)))\n",
        "                \n",
        "                # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
        "                # Need these as pandas dataframes to using binning scheme:\n",
        "                d = {'datetime':time_mat,'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(data_indexed)}\n",
        "                data_long = pd.DataFrame(data=d)\n",
        "                \n",
        "                # Bin data as averages across gridded spatial bins:\n",
        "                to_bin = lambda x: np.round(x / grid) * grid\n",
        "                data_long['latbins'] = data_long.lat.map(to_bin)\n",
        "                data_long['lonbins'] = data_long.lon.map(to_bin)\n",
        "                data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
        "                \n",
        "                # Rename binned columns + drop mean lat/lons:\n",
        "                data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
        "            # Save to output array\n",
        "            if n == 0: # for first iteration, define pre-allocated variable in dict as new binned data\n",
        "              U = data_proc\n",
        "            else: # for subsequent iterations, add to existing dict variable by \"stacking\" new binned datasets row-wise\n",
        "              U = pd.concat([U,data_proc]).groupby(['datetime','latbins','lonbins']).mean()\n",
        "            #-------------------------------------------------------------------\n",
        "            # V vector\n",
        "            vars_ = xr.open_dataset(filepath)\n",
        "            data = vars_.vwnd.values[0,:,:]\n",
        "            lat = vars_.lat.values\n",
        "            lon = vars_.lon.values\n",
        "            lon = pd.Series(lon).where(lon<180, lon-360).values # convert to W/E coords\n",
        "            time = int(pd.to_datetime(vars_.time.values).strftime('%m').values)\n",
        "            if lon[1]-lon[0] >= grid: #i.e. if upsampling to finer grid size\n",
        "                # Regrid data and interpolate though:\n",
        "                # First find the indices to index by lat/lon\n",
        "                latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "                loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "\n",
        "                # Restrict the data to the specified lat/lons\n",
        "                data_indexed = data[latinds[:,None], loninds[None,:]][:,:,0]\n",
        "\n",
        "                # Create data matrix with coordinates\n",
        "                data_mat = pd.DataFrame(data_indexed)\n",
        "                data_mat.columns = lon[loninds].flatten()\n",
        "                data_mat.index = lat[latinds].flatten()\n",
        "                #sort the data so that columns are ascending (matches 'new_shape' below)\n",
        "                data_mat = data_mat.reindex(sorted(data_mat.columns), axis=1) # sort columns in ascending order\n",
        "\n",
        "                # Create a new matrix with new gridded coordinates\n",
        "                lat_new = np.arange(min_lat, max_lat+grid, grid)\n",
        "                lon_new = np.arange(min_lon, max_lon+grid, grid)\n",
        "                new_shape = pd.DataFrame(np.ones((lat_new.shape[0],lon_new.shape[0])))\n",
        "                new_shape.columns=lon_new\n",
        "                new_shape.index=lat_new\n",
        "\n",
        "                # Now reindex data to new coordinates - add in NaNs to interpolate through\n",
        "                # important: find corresponding nearest value in new index/columns to replace data coords by\n",
        "                new_idx = [new_shape.index[abs(new_shape.index.values-i).argmin()] for i in data_mat.index]\n",
        "                new_cols = [new_shape.columns[abs(new_shape.columns.values-i).argmin()] for i in data_mat.columns]\n",
        "                # now rename idx/cols with new values in the data matrix (this allows the reindex_like function to properly map and insert nans below)\n",
        "                data_mat.set_axis(new_idx, axis=0, inplace=True)\n",
        "                data_mat.set_axis(new_cols, axis=1, inplace=True)\n",
        "\n",
        "                data_proc = pd.DataFrame(data_mat.reindex_like(new_shape).stack(dropna=False))\n",
        "                data_proc = data_proc.rename(columns={0:'data'})\n",
        "                data_proc.index = data_proc.index.set_names(['latbins','lonbins'])\n",
        "                data_proc['datetime'] = np.tile(time,data_proc.shape[0])\n",
        "                data_proc.reset_index(inplace=True)\n",
        "                data_proc.set_index(['datetime','latbins','lonbins'], inplace=True)\n",
        "                # data_proc.reset_index(inplace=True)\n",
        "                \n",
        "            #-----------------------------------------------------------------\n",
        "            # case 2: downsample to courser grid\n",
        "            if lon[1]-lon[0] < grid: # i.e. if interpolating to courser grid\n",
        "                # Bin the data\n",
        "                \n",
        "                # First find the indices to index by lat/lon\n",
        "                latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "                loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "                \n",
        "                # Restrict the data to the NE Pacific lat/lons\n",
        "                data_indexed = data[latinds[:,None], loninds[None,:]][:,:,0]\n",
        "                \n",
        "                # Convert time/lats/lons into repeating matrices to match data dimensions\n",
        "                lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
        "                lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
        "                time_mat = np.tile(time, len(np.ravel(lat_mat)))\n",
        "                \n",
        "                # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
        "                # Need these as pandas dataframes to using binning scheme:\n",
        "                d = {'datetime':time_mat,'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(data_indexed)}\n",
        "                data_long = pd.DataFrame(data=d)\n",
        "                \n",
        "                # Bin data as averages across gridded spatial bins:\n",
        "                to_bin = lambda x: np.round(x / grid) * grid\n",
        "                data_long['latbins'] = data_long.lat.map(to_bin)\n",
        "                data_long['lonbins'] = data_long.lon.map(to_bin)\n",
        "                data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
        "                \n",
        "                # Rename binned columns + drop mean lat/lons:\n",
        "                data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
        "            # Save to output array\n",
        "            if n == 0: # for first iteration, define pre-allocated variable in dict as new binned data\n",
        "              V = data_proc\n",
        "            else: # for subsequent iterations, add to existing dict variable by \"stacking\" new binned datasets row-wise\n",
        "              V = pd.concat([V,data_proc]).groupby(['datetime','latbins','lonbins']).mean()\n",
        "              \n",
        "  #=============================================================================\n",
        "  # write data to file\n",
        "  from google.colab import files\n",
        "  wind.to_csv(write_to+'wind_CCMP'+'_'+str(round(grid,3))+'_deg.csv')\n",
        "  U.to_csv(write_to+'U_CCMP'+'_'+str(round(grid,3))+'_deg.csv')\n",
        "  V.to_csv(write_to+'V_CCMP'+'_'+str(round(grid,3))+'_deg.csv')\n",
        "  files.download(write_to+'wind_CCMP'+'_'+str(round(grid,3))+'_deg.csv')\n",
        "  files.download(write_to+'U_CCMP'+'_'+str(round(grid,3))+'_deg.csv')\n",
        "  files.download(write_to+'V_CCMP'+'_'+str(round(grid,3))+'_deg.csv')\n",
        "else: # Fe data doesn't date back past 2007, so ignore this cell\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_1e9bba39-46c5-40d8-93e1-90abc1ecb34d\", \"SO_DMS_wind_CCMP_0.18_deg.csv\", 169144291)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_822b4126-9d2b-4d56-a6f4-edfc1de35d84\", \"SO_DMS_U_CCMP_0.18_deg.csv\", 169322772)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_dd0e335a-af3c-45e7-b5c0-e06612757eeb\", \"SO_DMS_V_CCMP_0.18_deg.csv\", 170475215)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxhm9l5PkRhB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "bf137a0d-7629-470f-e9eb-4421f5730f1f"
      },
      "source": [
        "#%% Processing WOA18 data (csv; 1x1 to 0.25x0.25)\n",
        "\n",
        "print('\\nProcessing WOA18 SSN data...')\n",
        "print('\\nLoading data...')\n",
        "# if DMS\n",
        "if which_dataset == 'DMS':\n",
        "  months = [1,2,3,4,10,11,12]\n",
        "else: # if Fe\n",
        "  months = [2,3,4,5,6,7,8,9,10,11,12]\n",
        "#------------------------------------------------------------------------------\n",
        "#### Nitrate\n",
        "#------------------------------------------------------------------------------\n",
        "# Pre-allocate dask array to store new binned data to\n",
        "SSN = dd.from_array(da.empty((1,len(months))))\n",
        "\n",
        "for n,pt in enumerate(months):\n",
        "    # Load up wind data file (Copernicus timeseries data is downloaded into a single netCDF)\n",
        "    if pt<10:\n",
        "      data = pd.read_csv('WOA18/woa18_all_n0'+str(pt)+'an01.csv', header=[1]).reset_index().drop('index', axis=1).rename(columns={'#COMMA SEPARATED LATITUDE':'lat',' LONGITUDE':'lon', ' AND VALUES AT DEPTHS (M):0':0}).set_index(['lat','lon']).loc[:,0]\n",
        "    else:\n",
        "      data = pd.read_csv('WOA18/woa18_all_n'+str(pt)+'an01.csv', header=[1]).reset_index().drop('index', axis=1).rename(columns={'#COMMA SEPARATED LATITUDE':'lat',' LONGITUDE':'lon', ' AND VALUES AT DEPTHS (M):0':0}).set_index(['lat','lon']).loc[:,0]\n",
        "    lon = data.index.get_level_values('lon')\n",
        "    lat = data.index.get_level_values('lat')\n",
        "    data = data.unstack()\n",
        "    time = pd.to_datetime({'year': [2000],'month': [pt],'day': [1]})\n",
        "    \n",
        "    #-----------------------------------------------------------------\n",
        "    # case 1: match pixels to high resolution satellite data\n",
        "    if lon[1]-lon[0] >= grid: #i.e. if upsampling to finer grid size\n",
        "        # Regrid data and interpolate though:\n",
        "        # First find the indices to index by lat/lon\n",
        "        latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "        loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "\n",
        "        # Restrict the data to the specified lat/lons\n",
        "        data_indexed = data.loc[min_lat:max_lat, min_lon:max_lon]\n",
        "\n",
        "        # Create data matrix with coordinates\n",
        "        data_mat = pd.DataFrame(data_indexed)\n",
        "        # data_mat.columns = lon[min_lon:max_lon]\n",
        "        # data_mat.index = lat[min_lat:max_lat]\n",
        "        #sort the data so that columns are ascending (matches 'new_shape' below)\n",
        "        data_mat = data_mat.reindex(sorted(data_mat.columns), axis=1) # sort columns in ascending order\n",
        "\n",
        "        # Create a new matrix with new gridded coordinates\n",
        "        lat_new = np.arange(min_lat, max_lat+grid, grid)\n",
        "        lon_new = np.arange(min_lon, max_lon+grid, grid)\n",
        "        new_shape = pd.DataFrame(np.ones((lat_new.shape[0],lon_new.shape[0])))\n",
        "        new_shape.columns=lon_new\n",
        "        new_shape.index=lat_new\n",
        "\n",
        "        # Now reindex data to new coordinates - add in NaNs to interpolate through\n",
        "        # important: find corresponding nearest value in new index/columns to replace data coords by\n",
        "        new_idx = [new_shape.index[abs(new_shape.index.values-i).argmin()] for i in data_mat.index]\n",
        "        new_cols = [new_shape.columns[abs(new_shape.columns.values-i).argmin()] for i in data_mat.columns]\n",
        "        # now rename idx/cols with new values in the data matrix (this allows the reindex_like function to properly map and insert nans below)\n",
        "        data_mat.set_axis(new_idx, axis=0, inplace=True)\n",
        "        data_mat.set_axis(new_cols, axis=1, inplace=True)\n",
        "\n",
        "        data_proc = pd.DataFrame(data_mat.reindex_like(new_shape).stack(dropna=False))\n",
        "        data_proc = data_proc.rename(columns={0:'data'})\n",
        "        data_proc.index = data_proc.index.set_names(['latbins','lonbins'])\n",
        "        data_proc['datetime'] = np.tile(pt,data_proc.shape[0])\n",
        "        data_proc.reset_index(inplace=True)\n",
        "        data_proc.set_index(['datetime','latbins','lonbins'], inplace=True)\n",
        "        # data_proc.reset_index(inplace=True)\n",
        "        \n",
        "    #-----------------------------------------------------------------\n",
        "    # case 2: downsample to courser grid\n",
        "    if lon[1]-lon[0] < grid: # i.e. if interpolating to courser grid\n",
        "        # Bin the data\n",
        "        \n",
        "        # First find the indices to index by lat/lon\n",
        "        latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "        loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "        \n",
        "        # Restrict the data to the NE Pacific lat/lons\n",
        "        data_indexed = data.loc[min_lat:max_lat, min_lon:max_lon]\n",
        "        \n",
        "        # Convert time/lats/lons into repeating matrices to match data dimensions\n",
        "        lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
        "        lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
        "        time_mat = np.tile(pt, len(np.ravel(lat_mat)))\n",
        "        \n",
        "        # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
        "        # Need these as pandas dataframes to use binning scheme:\n",
        "        d = {'datetime':time_mat,'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(data_indexed)}\n",
        "        data_long = pd.DataFrame(data=d)\n",
        "        \n",
        "        # Bin data as averages across gridded spatial bins:\n",
        "        to_bin = lambda x: np.round(x / grid) * grid\n",
        "        data_long['latbins'] = data_long.lat.map(to_bin)\n",
        "        data_long['lonbins'] = data_long.lon.map(to_bin)\n",
        "        data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
        "        \n",
        "        # Rename binned columns + drop mean lat/lons:\n",
        "        data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
        "    # Save to output array\n",
        "    if n == 0: # for first iteration, define pre-allocated variable in dict as new binned data\n",
        "        SSN = data_proc\n",
        "    else: # for subsequent iterations, add to existing dict variable by \"stacking\" new binned datasets row-wise\n",
        "        SSN = pd.concat([SSN,data_proc]).groupby(['datetime','latbins','lonbins']).mean()\n",
        "    print(str(n+1)+' of '+str(np.size(months))+' iterations completed!')\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "#### Silicate\n",
        "#------------------------------------------------------------------------------\n",
        "# Pre-allocate dask array to store new binned data to\n",
        "Si = dd.from_array(da.empty((1,len(months))))\n",
        "\n",
        "for n,pt in enumerate(months):\n",
        "    # Load up wind data file (Copernicus timeseries data is downloaded into a single netCDF)\n",
        "    if pt<10:\n",
        "      data = pd.read_csv('WOA18/woa18_all_i0'+str(pt)+'an01.csv', header=[1]).reset_index().drop('index', axis=1).rename(columns={'#COMMA SEPARATED LATITUDE':'lat',' LONGITUDE':'lon', ' AND VALUES AT DEPTHS (M):0':0}).set_index(['lat','lon']).loc[:,0]\n",
        "    else:\n",
        "      data = pd.read_csv('WOA18/woa18_all_i'+str(pt)+'an01.csv', header=[1]).reset_index().drop('index', axis=1).rename(columns={'#COMMA SEPARATED LATITUDE':'lat',' LONGITUDE':'lon', ' AND VALUES AT DEPTHS (M):0':0}).set_index(['lat','lon']).loc[:,0]\n",
        "    lon = data.index.get_level_values('lon')\n",
        "    lat = data.index.get_level_values('lat')\n",
        "    data = data.unstack()\n",
        "    time = pd.to_datetime({'year': [2000],'month': [pt],'day': [1]})\n",
        "    \n",
        "    #-----------------------------------------------------------------\n",
        "    # case 1: match pixels to high resolution satellite data\n",
        "    if lon[1]-lon[0] >= grid: #i.e. if upsampling to finer grid size\n",
        "        # Regrid data and interpolate though:\n",
        "        # First find the indices to index by lat/lon\n",
        "        latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "        loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "\n",
        "        # Restrict the data to the specified lat/lons\n",
        "        data_indexed = data.loc[min_lat:max_lat, min_lon:max_lon]\n",
        "\n",
        "        # Create data matrix with coordinates\n",
        "        data_mat = pd.DataFrame(data_indexed)\n",
        "        # data_mat.columns = lon[min_lon:max_lon]\n",
        "        # data_mat.index = lat[min_lat:max_lat]\n",
        "        #sort the data so that columns are ascending (matches 'new_shape' below)\n",
        "        data_mat = data_mat.reindex(sorted(data_mat.columns), axis=1) # sort columns in ascending order\n",
        "\n",
        "        # Create a new matrix with new gridded coordinates\n",
        "        lat_new = np.arange(min_lat, max_lat+grid, grid)\n",
        "        lon_new = np.arange(min_lon, max_lon+grid, grid)\n",
        "        new_shape = pd.DataFrame(np.ones((lat_new.shape[0],lon_new.shape[0])))\n",
        "        new_shape.columns=lon_new\n",
        "        new_shape.index=lat_new\n",
        "\n",
        "        # Now reindex data to new coordinates - add in NaNs to interpolate through\n",
        "        # important: find corresponding nearest value in new index/columns to replace data coords by\n",
        "        new_idx = [new_shape.index[abs(new_shape.index.values-i).argmin()] for i in data_mat.index]\n",
        "        new_cols = [new_shape.columns[abs(new_shape.columns.values-i).argmin()] for i in data_mat.columns]\n",
        "        # now rename idx/cols with new values in the data matrix (this allows the reindex_like function to properly map and insert nans below)\n",
        "        data_mat.set_axis(new_idx, axis=0, inplace=True)\n",
        "        data_mat.set_axis(new_cols, axis=1, inplace=True)\n",
        "\n",
        "        data_proc = pd.DataFrame(data_mat.reindex_like(new_shape).stack(dropna=False))\n",
        "        data_proc = data_proc.rename(columns={0:'data'})\n",
        "        data_proc.index = data_proc.index.set_names(['latbins','lonbins'])\n",
        "        data_proc['datetime'] = np.tile(pt,data_proc.shape[0])\n",
        "        data_proc.reset_index(inplace=True)\n",
        "        data_proc.set_index(['datetime','latbins','lonbins'], inplace=True)\n",
        "        # data_proc.reset_index(inplace=True)\n",
        "        \n",
        "    #-----------------------------------------------------------------\n",
        "    # case 2: downsample to courser grid\n",
        "    if lon[1]-lon[0] < grid: # i.e. if interpolating to courser grid\n",
        "        # Bin the data\n",
        "        \n",
        "        # First find the indices to index by lat/lon\n",
        "        latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "        loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "        \n",
        "        # Restrict the data to the NE Pacific lat/lons\n",
        "        data_indexed = data.loc[min_lat:max_lat, min_lon:max_lon]\n",
        "        \n",
        "        # Convert time/lats/lons into repeating matrices to match data dimensions\n",
        "        lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
        "        lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
        "        time_mat = np.tile(pt, len(np.ravel(lat_mat)))\n",
        "        \n",
        "        # Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
        "        # Need these as pandas dataframes to using binning scheme:\n",
        "        d = {'datetime':time_mat,'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(data_indexed)}\n",
        "        data_long = pd.DataFrame(data=d)\n",
        "        \n",
        "        # Bin data as averages across gridded spatial bins:\n",
        "        to_bin = lambda x: np.round(x / grid) * grid\n",
        "        data_long['latbins'] = data_long.lat.map(to_bin)\n",
        "        data_long['lonbins'] = data_long.lon.map(to_bin)\n",
        "        data_proc = data_long.groupby(['datetime', 'latbins', 'lonbins']).mean()\n",
        "        \n",
        "        # Rename binned columns + drop mean lat/lons:\n",
        "        data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
        "    # Save to output array\n",
        "    if n == 0: # for first iteration, define pre-allocated variable in dict as new binned data\n",
        "        Si = data_proc\n",
        "    else: # for subsequent iterations, add to existing dict variable by \"stacking\" new binned datasets row-wise\n",
        "        Si = pd.concat([Si,data_proc]).groupby(['datetime','latbins','lonbins']).mean()\n",
        "    print(str(n+1)+' of '+str(np.size(months))+' iterations completed!')\n",
        "\n",
        "#===============================================================================\n",
        "# write data to file\n",
        "from google.colab import files\n",
        "SSN.to_csv(write_to+'SSN'+'_'+str(round(grid,3))+'_deg.csv')\n",
        "Si.to_csv(write_to+'Si'+'_'+str(round(grid,3))+'_deg.csv')\n",
        "files.download(write_to+'SSN'+'_'+str(round(grid,3))+'_deg.csv')\n",
        "files.download(write_to+'Si'+'_'+str(round(grid,3))+'_deg.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing WOA18 SSN data...\n",
            "\n",
            "Loading data...\n",
            "1 of 7 iterations completed!\n",
            "2 of 7 iterations completed!\n",
            "3 of 7 iterations completed!\n",
            "4 of 7 iterations completed!\n",
            "5 of 7 iterations completed!\n",
            "6 of 7 iterations completed!\n",
            "7 of 7 iterations completed!\n",
            "1 of 7 iterations completed!\n",
            "2 of 7 iterations completed!\n",
            "3 of 7 iterations completed!\n",
            "4 of 7 iterations completed!\n",
            "5 of 7 iterations completed!\n",
            "6 of 7 iterations completed!\n",
            "7 of 7 iterations completed!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_8114bc2f-3def-4522-a15f-fcfd8a86f2a6\", \"SO_DMS_SSN_0.18_deg.csv\", 162137611)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_da1e7fbf-6f1f-443a-9a3d-61ca8ff448a4\", \"SO_DMS_Si_0.18_deg.csv\", 162115379)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "jP3rDwRRDLP5",
        "outputId": "f934d3ea-c01c-4468-f304-ff82dbb433f0"
      },
      "source": [
        "#%% ETOPO2 data\n",
        "# This bathymetry data is used to mask out land pixels when interpolating\n",
        "vars_ = xr.open_dataset('drive/MyDrive/etopo2.nc')\n",
        "data = vars_.btdata.values\n",
        "lat = vars_.coords['lat'].values\n",
        "lon = vars_.coords['lon'].values\n",
        "#-----------------------------------------------------------------\n",
        "# Bin the data\n",
        "\n",
        "# First find the indices to index by lat/lon\n",
        "latinds = np.argwhere((lat >= min_lat) & (lat <= max_lat)).astype(int)\n",
        "loninds = np.argwhere((lon >= min_lon) & (lon <= max_lon)).astype(int)\n",
        "\n",
        "# Restrict the data to the NE Pacific lat/lons\n",
        "data_indexed = data[latinds[:,None], loninds[None,:]][:,:,0]\n",
        "\n",
        "# Convert time/lats/lons into repeating matrices to match data dimensions\n",
        "lat_mat = np.tile(lat[latinds], len(lon[loninds])) # latitude is repeated by column\n",
        "lon_mat = np.tile(lon[loninds], len(lat[latinds])).T # transpose to repeat longitude by row\n",
        "\n",
        "# Now create a new matrix with the unravel matrices into vectors - np.ravel does this row-by-row\n",
        "# Need these as pandas dataframes to using binning scheme:\n",
        "d = {'lat':np.ravel(lat_mat),'lon':np.ravel(lon_mat),'data':np.ravel(data_indexed)}\n",
        "data_long = pd.DataFrame(data=d)\n",
        "\n",
        "# Bin data as averages across gridded spatial bins:\n",
        "to_bin = lambda x: np.round(x / grid) * grid\n",
        "data_long['latbins'] = data_long.lat.map(to_bin)\n",
        "data_long['lonbins'] = data_long.lon.map(to_bin)\n",
        "data_proc = data_long.groupby(['latbins', 'lonbins']).mean()\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "# Rename binned columns + drop mean lat/lons:\n",
        "data_proc = data_proc.drop(columns=['lat', 'lon'])\n",
        "data_proc.reset_index(inplace=True) # remove index specification on columns\n",
        "# etopo = data_proc.iloc[:,1:]\n",
        "etopo = data_proc\n",
        "etopo.set_index(['lonbins','latbins'], inplace=True)\n",
        "# del data_proc, vars_, data_long\n",
        "#===============================================================================\n",
        "# Write data to file\n",
        "from google.colab import files\n",
        "etopo.to_csv('SO_etopo'+'_'+str(round(grid,3))+'_deg.csv')\n",
        "files.download('SO_etopo'+'_'+str(round(grid,3))+'_deg.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_1812998f-2183-42f9-bcf0-98f063489a4b\", \"SO_etopo_0.18_deg.csv\", 25819055)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3F572WUVmfm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
